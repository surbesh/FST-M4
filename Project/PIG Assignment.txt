PIG FILE NAME: episodeIV.pig
-- Load input file from HDFS
inputFile = LOAD 'hdfs:///user/project/episode4.txt' USING PigStorage('\t') AS (name:chararray, line:chararray);
-- Add line number to each line in the file.
ranked = RANK inputFile;
-- Filter to read from third line as first two lines are not required.
onlyDialogues = FILTER ranked BY (rank_inputFile > 2);
-- Group by name
groupByName = GROUP onlyDialogues BY name;
-- Group names and number of lines.
names = FOREACH groupByName GENERATE $0 as name, COUNT($1) as no_of_lines;
-- Order the names in desceding order of number of lines
namesOrdered = ORDER names BY no_of_lines DESC;
-- Store the data in output file
STORE namesOrdered INTO 'hdfs:///user/project/episode4output' USING PigStorage('\t');

pig episode4.pig
hdfs dfs -cat /user/project/episode4output/part-r-00000

## Command to copy file from HDFS to Container.
root@895fd9c08e12:/# hdfs dfs -get 'hdfs:///user/project/episode4output' ./episode4output
## 895fd9c08e12 is current Container ID
## Command to copy file from Container to Local
docker cp 895fd9c08e12:/episode4output 'C:\Module4\Hadoop\episode4_output.txt'

PIG FILE NAME: episodeV.pig
-- Load input file from HDFS
inputFile = LOAD 'hdfs:///user/project/episode5.txt' USING PigStorage('\t') AS (name:chararray, line:chararray);
-- Add line number to each line in the file.
ranked = RANK inputFile;
-- Filter to read from third line as first two lines are not required.
onlyDialogues = FILTER ranked BY (rank_inputFile > 2);
-- Group by name
groupByName = GROUP onlyDialogues BY name;
-- Group names and number of lines.
names = FOREACH groupByName GENERATE $0 as name, COUNT($1) as no_of_lines;
-- Order the names in desceding order of number of lines
namesOrdered = ORDER names BY no_of_lines DESC;
-- Store the data in output file
STORE namesOrdered INTO 'hdfs:///user/project/episode5output' USING PigStorage('\t');

/usr/local/hadoop-3.3.1/sbin/mr-jobhistory-daemon.sh start historyserver
pig episode5.pig
hdfs dfs -cat /user/project/episode5output/part-r-00000
## Command to copy file from HDFS to Container.
root@895fd9c08e12:/# hdfs dfs -get 'hdfs:///user/project/episode5output' ./episode5output
## 895fd9c08e12 is current Container ID
## Command to copy file from Container to Local
docker cp 895fd9c08e12:/episode5output 'C:\Module4\Hadoop\episode5_output'

PIG FILE NAME: episodeVI.pig
-- Load input file from HDFS
inputFile = LOAD 'hdfs:///user/project/episode6.txt' USING PigStorage('\t') AS (name:chararray, line:chararray);
-- Add line number to each line in the file.
ranked = RANK inputFile;
-- Filter to read from third line as first two lines are not required.
onlyDialogues = FILTER ranked BY (rank_inputFile > 2);
-- Group by name
groupByName = GROUP onlyDialogues BY name;
-- Group names and number of lines.
names = FOREACH groupByName GENERATE $0 as name, COUNT($1) as no_of_lines;
-- Order the names in desceding order of number of lines
namesOrdered = ORDER names BY no_of_lines DESC;
-- Store the data in output file
STORE namesOrdered INTO 'hdfs:///user/project/episode6output' USING PigStorage('\t');

/usr/local/hadoop-3.3.1/sbin/mr-jobhistory-daemon.sh start historyserver
pig episode6.pig
hdfs dfs -cat /user/project/episode6output/part-r-00000
## Command to copy file from HDFS to Container.
root@895fd9c08e12:/# hdfs dfs -get 'hdfs:///user/project/episode6output' ./episode6output
## 895fd9c08e12 is current Container ID
## Command to copy file from Container to Local
docker cp 895fd9c08e12:/episode6output 'C:\Module4\Hadoop\episode6_output'